{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jJlkeHIb2foH",
        "ISi76YZ9VHiN",
        "XJWivRFrZR3v",
        "DuxcceFU60jK",
        "Sd9ePeQs64NE",
        "9QR1nWqHdml9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jo18kaCJ0ahH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, let's initialize some low-dimensional (ground truth) data in a high-dimensional space..."
      ],
      "metadata": {
        "id": "jJlkeHIb2foH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# true dimensions of interest\n",
        "# NOTE: in high dimensions, randomly chosen directions tend to be nearly orthogonal\n",
        "\n",
        "num_data = 1_000    # number of data points (e.g. time steps in a recording)\n",
        "dim = 100           # number of dimensions in data (e.g. number of neurons in the recording)\n",
        "\n",
        "# generate two ground truth directions that data should fall along, up to slight (noisy) perturbations\n",
        "v1 = np.random.normal(size=dim)\n",
        "v1 /= np.linalg.norm(v1)\n",
        "\n",
        "v2 = np.random.normal(size=dim)\n",
        "v2 /= np.linalg.norm(v2)\n",
        "\n",
        "noise_std = 0.05\n",
        "\n",
        "# generate the data\n",
        "X = np.zeros((num_data, dim))\n",
        "for i in range(num_data):\n",
        "    noise_vec = noise_std * np.random.normal(size=dim)\n",
        "\n",
        "    if i < num_data / 2:\n",
        "        X[i, :] = v1 * np.random.normal() + noise_vec\n",
        "    else:\n",
        "        X[i, :] = v2 * np.random.normal() + noise_vec\n",
        "\n",
        "\n",
        "print(f'Number of data points: {num_data}')\n",
        "print(f'Number of dimensions: {dim}')\n"
      ],
      "metadata": {
        "id": "Qapo-mPW2dqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafe7bec-11a3-4c5b-f0d8-581bbac5dd07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points: 1000\n",
            "Number of dimensions: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do we suspect that the data is low dimensional?\n",
        "\n",
        "We can quantify this by first computing the covariance matrix $C = \\frac1n X^T X \\in \\mathbb{R}^{\\text{num_features } \\times \\text{ num_features}}$ (note that $X \\in \\mathbb{R}^{\\text{num_data } \\times \\text{ num_features}}$), and then calculating its **participation ratio** $\\rho = \\frac{(\\text{Tr}(C))^2}{\\text{Tr}(C^2)}$.\n",
        "\n",
        "Here, the trace operator $\\text{Tr}$ is the (linear) function that takes in an $N \\times N$ matrix and outputs the sum of the diagonal elements of that matrix: $\\text{Tr}(A) = \\sum_{i=1}^N A_{ii}$. Interestingly, it turns out that the trace of a matrix is *also* the sum of its eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_N$."
      ],
      "metadata": {
        "id": "ISi76YZ9VHiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Show that the eigenvalues of $C^2$ are $\\lambda_1^2, \\dots, \\lambda_N^2$. Using this, and the fact that the trace operator returns the sum of the eigenvalues, show that $\\rho = \\frac{\\left(\\sum_{k=1}^N \\lambda_k\\right)^2}{\\sum_{k=1}^N \\lambda_k^2}$."
      ],
      "metadata": {
        "id": "fa1UiwlrW3YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collectively, the number of nonzero eigenvalues and their relative sizes tells us how much the data features tend to vary among the different eigenvector directions. In particular, if a lot of eigenvalues are zero, or close to zero, we should expect the data to be low-dimensional!\n",
        "\n",
        "**Question:** Suppose that $\\lambda_1 = \\lambda_2 = \\dots = \\lambda_N$. What does $\\rho$ equal in this case? Now, suppose that $\\lambda_1 \\neq 0$ but that $\\lambda_2 = \\lambda_3 = \\dots = \\lambda_N = 0$; what is $\\rho$ in this case? What if $\\lambda_1 = \\lambda_2 \\neq 0$ and all other eigenvalues are zero?\n",
        "\n",
        "Do you see how a lower participation ratio $\\rho$ indicates lower data dimensionality?"
      ],
      "metadata": {
        "id": "de--6WqAXPEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, compute the participation ratio of the data covariance matrix! Is it large or small relative to the total dimensionality $N$? Does this indicate we should try using PCA?"
      ],
      "metadata": {
        "id": "XJWivRFrZR3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ADD CODE HERE\n",
        "# Hint: np.trace() is the numpy function that calculates the trace of a square matrix!\n",
        "\n",
        "# C = ...\n",
        "\n",
        "# rho = ...\n",
        "\n",
        "# print results\n",
        "print(f'The participation ratio is {rho}, and the total data dimensionality is {X.shape[1]}.')\n"
      ],
      "metadata": {
        "id": "3hIh8yiXZHgY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, compute the covariance matrix of the data, and compute its SVD / eigendecomposition!"
      ],
      "metadata": {
        "id": "DuxcceFU60jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ADD CODE HERE\n",
        "# Hint: there's lots of functions for computing the eigendecomposition and/or SVD.\n",
        "# There's np.linalg.eigh() (among others), and also some analogous scipy functions.\n",
        "\n",
        "# C = ...\n",
        "\n",
        "# U = ...\n",
        "\n",
        "# top_eigvals = ...\n",
        "\n",
        "print(U.shape)\n",
        "print(top_eigvals)\n"
      ],
      "metadata": {
        "id": "NPmdcHVF5FCW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally, take the top two eigenvectors and project the data onto their span!"
      ],
      "metadata": {
        "id": "Sd9ePeQs64NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADD CODE HERE\n",
        "\n",
        "# X_proj = ...  (the shape should be num_data x 2)\n",
        "\n",
        "print(X_proj.shape)\n",
        "\n",
        "# plot first two PC's of the data, along with\n",
        "# the two ground truth direction vectors\n",
        "\n",
        "v1_proj = U.T @ v1\n",
        "v2_proj = U.T @ v2\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(X_proj[:, 0], X_proj[:, 1], 'o', zorder=1)\n",
        "\n",
        "plt.quiver(0, 0, v1_proj[0], v1_proj[1],\n",
        "           angles='xy', scale_units='xy', scale=1,\n",
        "           color='r', width=0.005, headwidth=5, headlength=7, zorder=3)\n",
        "\n",
        "plt.quiver(0, 0, v2_proj[0], v2_proj[1],\n",
        "           angles='xy', scale_units='xy', scale=1,\n",
        "           color='darkgreen', width=0.005, headwidth=5, headlength=7, zorder=3)\n",
        "\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.xlabel('PC1', fontsize=16)\n",
        "plt.ylabel('PC2', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KVKZa1GH6SoF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How much of the variance did we explain with two PC's?\n",
        "\n"
      ],
      "metadata": {
        "id": "9QR1nWqHdml9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that, intuitively, the sizes of the eigenvalues of the covariance matrix $C = \\frac1n X^T X$ (where $n$ is the number of data) tell us to what extent the data varies along the different eigenvector directions. By using more principal components, we capture more and more of the variance along different eigen-directions."
      ],
      "metadata": {
        "id": "AQFE5qN5d1Zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate what proportion of the variance in the data is accounted for with $k \\leq N$ principal components, we arrange the eigenvalues in decreasing order as $\\lambda_1, \\lambda_2, \\dots, \\lambda_N$ and then compute the quantity $\\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^N \\lambda_i}$. Let us now compute the proportion of the variance explained in our present case:"
      ],
      "metadata": {
        "id": "mshWAWmIecuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADD CODE HERE\n",
        "\n",
        "# top_2_eigvals = ...\n",
        "\n",
        "# total_eigval_sum = ...\n",
        "\n",
        "# prop_var_explained = ...\n",
        "\n",
        "print(f'Proportion of total variance in data explained with 2 PCs: {prop_var_explained}.')\n"
      ],
      "metadata": {
        "id": "BlZFQLDTfDCt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceptual Question: ** Where is the remaining variance coming from? Is it important to capture this remaining variance?"
      ],
      "metadata": {
        "id": "b93Lo0eLfsgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Congratulations, you are now a PCA wizard! Feel free to tinker around with any part of this exericse (e.g. the underlying data, the noise level, the number of PCs used, etc.) and see how the results change!"
      ],
      "metadata": {
        "id": "uly5ysVkf2H3"
      }
    }
  ]
}