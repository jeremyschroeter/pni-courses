{"cells":[{"cell_type":"code","execution_count":null,"id":"90f01d27","metadata":{"id":"90f01d27"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","id":"021685f6","metadata":{"id":"021685f6"},"source":["# Exercise 1 Sampling from distributions\n","Generate 10000 samples by `np.random.randn`"]},{"cell_type":"code","execution_count":null,"id":"055765e8","metadata":{"id":"055765e8"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","samples = ____ # TO DO: generate samples using NumPy"]},{"cell_type":"markdown","id":"f3c7dc9a","metadata":{"id":"f3c7dc9a"},"source":["Estimate the probability density function by the samples. Hint: use ` np.histogram ` and remember the PDF should integrate to 1."]},{"cell_type":"code","execution_count":null,"id":"efbfa9ee","metadata":{"id":"efbfa9ee"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","freq, bins = ____  # TODO: histogram with 100 bins, range -5 to 5\n","bin_size = ____    # TODO: compute bin size\n","pdf = ____         # TODO: normalize so area under curve = 1\n","xgrid = ____       # TODO: compute bin centers\n","____                # TODO: plot estimated PDF vs xgrid\n","plt.xlabel(\"x\")"]},{"cell_type":"markdown","id":"8bf124c4","metadata":{"id":"8bf124c4"},"source":["Compare your estimation with the ground true PDF of `np.random.randn`"]},{"cell_type":"code","execution_count":null,"id":"7cdfc6c9","metadata":{"id":"7cdfc6c9"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","def real_pdf(x):\n","    ____            # TODO: return standard normal PDF\n","\n","plt.plot(xgrid, pdf, label=\"estimated\")\n","____                # TODO: plot the true PDF curve\n","plt.legend()\n","plt.xlabel(\"x\")"]},{"cell_type":"markdown","id":"a8ffb207","metadata":{"id":"a8ffb207"},"source":["Show how the deviation between your estimation and ground true changes with the sample size"]},{"cell_type":"code","execution_count":null,"id":"2b5dcc33","metadata":{"id":"2b5dcc33"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","sample_sizes = [100, 500, 1000, 2000, 4000, 8000, 16000]\n","\n","for sample_size in sample_sizes:\n","    samples = ____  # TODO: generate samples\n","    freq, bins = ____  # TODO: histogram\n","    bin_size = ____    # TODO: compute bin size\n","    pdf = ____         # TODO: normalize PDF\n","    plt.plot(xgrid, pdf, label=f\"sample size = {sample_size}\")\n","\n","____  # TODO: plot true PDF again\n","plt.legend()\n","plt.xlabel(\"x\")"]},{"cell_type":"markdown","id":"32008634","metadata":{"id":"32008634"},"source":["# Exercise 2 Marginal & Conditional Probability Densitites"]},{"cell_type":"markdown","id":"9b13ed9b","metadata":{"id":"9b13ed9b"},"source":["Load the file `pmf2Dexample.mat`, which contains a discrete 2D probability mass function (PMF). The variables defined include:\n","\n","$\\quad$ `x` = a vector of $x$ points\n","\n","$\\quad$ `y` = a vector of $y$ points\n","\n","$\\quad$ `Pxy` = a 2D matrix, whose $i, j$ 'th entry is the probability $P(y = {\\tt y[i]}, x = {\\tt x[j]})$\n","\n","You can visualize this density in Python via the command:\n","`plt.imshow`"]},{"cell_type":"code","execution_count":null,"id":"6a94ceea","metadata":{"id":"6a94ceea"},"outputs":[],"source":["# Import a file from github using 'wget'\n","!wget https://github.com/pillowlab/neu314data/blob/main/pmf2Dexample.mat?raw=true\n","\n","# Rename raw files with the mv utility\n","!mv pmf2Dexample.mat\\?raw\\=true pmf2Dexample.mat"]},{"cell_type":"markdown","id":"935cbd6d","metadata":{"id":"935cbd6d"},"source":["if the above block doesn't work, try manually download the data file and put the data file in the same directory as this notebook"]},{"cell_type":"code","execution_count":null,"id":"00d0511c","metadata":{"id":"00d0511c"},"outputs":[],"source":["from scipy.io import loadmat\n","data = loadmat(\"pmf2Dexample.mat\")\n","Pxy = data[\"Pxy\"]\n","x = data[\"x\"]\n","y = data[\"y\"]"]},{"cell_type":"code","execution_count":null,"id":"faa26ee3","metadata":{"id":"faa26ee3"},"outputs":[],"source":["xlim = np.array([0, 20])  # axis limits for x variable\n","ylim = np.array([0, 25])  # axis limits for y variable\n","\n","plt.imshow(\n","    Pxy, aspect=\"auto\", origin=\"lower\", extent=[xlim[0], xlim[1], ylim[0], ylim[1]]\n",")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")"]},{"cell_type":"markdown","id":"c3c55d08","metadata":{"id":"c3c55d08"},"source":["From this joint two-dimensional distribution, compute and make plots (or images) of the following.\n","\n","\n"]},{"cell_type":"markdown","id":"e5a8bae5","metadata":{"id":"e5a8bae5"},"source":["$P(x)$ : The marginal probability distribution over $x$."]},{"cell_type":"code","execution_count":null,"id":"bdae949d","metadata":{"id":"bdae949d"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","# TODO: sum over y-axis (axis=0) to get Px\n","Px = ____\n","# TODO: plot Px vs x\n"]},{"cell_type":"markdown","id":"f048773b","metadata":{"id":"f048773b"},"source":["$P(y)$ : The marginal probability distribution over $y$."]},{"cell_type":"code","execution_count":null,"id":"acc96692","metadata":{"id":"acc96692"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","# TODO: sum over x-axis (axis=1) to get Py\n","Py = ____\n","# TODO: plot Py vs y\n"]},{"cell_type":"markdown","id":"13e33bba","metadata":{"id":"13e33bba"},"source":["$P(y\\,|\\,x=5)$ : the conditional over $y$ given $x=5$"]},{"cell_type":"code","execution_count":null,"id":"e1dc0851","metadata":{"id":"e1dc0851"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","# TODO: find index of x closest to 5\n","def find_index(x_value, x):\n","    return ____\n","\n","x_condition = 5\n","col_idx = find_index(x_condition, x)\n","\n","# TODO: select column from Pxy for this x and normalize it\n","Pycx = ____\n","\n","# TODO: plot Pycx vs y\n"]},{"cell_type":"markdown","id":"4b1d5a39","metadata":{"id":"4b1d5a39"},"source":[" $P(x\\,|\\,y)$ : the full image of the conditional density $P(x\\,|\\,y)$"]},{"cell_type":"code","execution_count":null,"id":"08edd740","metadata":{"id":"08edd740"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","# TODO: tile Py so it matches shape of Pxy for division\n","Py_tile = ____\n","\n","# TODO: compute P(x|y) as Pxy divided by Py_tile\n","P_x_given_y = ____\n","\n","# TODO: plot P(x|y) as an image with imshow, using same extent/aspect/origin as before\n"]},{"cell_type":"markdown","metadata":{"id":"YXGbFP2XA2xY"},"source":["# Exercise 3 Mean, Variance, Independence"],"id":"YXGbFP2XA2xY"},{"cell_type":"markdown","source":["Two common statistics one might wish to compute from a distribution are its mean and variance.  \n","\n","- The *mean* is the mean or average value, given by $\\mathbb{E}[x] = \\int x P(x) dx$ when $P(x)$ is a pdf, and $\\mathbb{E}[x] = \\sum_{i} x_i P(x_i)$ when $P(x)$ is a pmf.\n","- The *variance* is the mean value of $x$ minus its mean squared:  $\\mathrm{var}(x) = \\mathbb{E}[(x - \\mathbb{E}[x])^2] = \\sum_i (x_i - \\bar x)^2 P(x_i)$\n"],"metadata":{"id":"FJQY5Xw2BAbI"},"id":"FJQY5Xw2BAbI"},{"cell_type":"markdown","source":["Compute the mean and variance of the marginal distribution $P(x)$.\n"],"metadata":{"id":"esEJh8JiBFRI"},"id":"esEJh8JiBFRI"},{"cell_type":"code","source":["# TODO: Your code here!\n","mean_x = ____\n","variance_x = ____\n","\n","print(f\"Mean of P(x): {mean_x}\")\n","print(f\"Variance of P(x): {variance_x}\")"],"metadata":{"id":"crjGdJ0BA4TQ"},"id":"crjGdJ0BA4TQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute the mean and variance of the conditional distribution $P(x | y = 3)$."],"metadata":{"id":"JvMivAorBTxI"},"id":"JvMivAorBTxI"},{"cell_type":"code","source":["# TODO: Your code here!\n","y_target = 3\n","y_index = ____ # TODO: find index of y closest to y_target\n","\n","Px_given_y3 = ____ # TODO: extract conditional Px|y=3 from Pxy\n","\n","# TODO: normalize Px_given_y3 so it sums to 1\n","____\n","\n","# TODO: compute mean_x_given_y3\n","mean_x_given_y3 = ____\n","\n","# TODO: compute variance_x_given_y3\n","variance_x_given_y3 = ____\n","\n","print(f\"Mean of P(x | y=3): {mean_x_given_y3}\")\n","print(f\"Variance of P(x | y=3): {variance_x_given_y3}\")\n"],"metadata":{"id":"FOESykmlBYMA"},"id":"FOESykmlBYMA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It is clear from the image of $P(x,y)$ shown above that $x$ and $y$ are correlated, and thus $P(x,y)$ is not independent. However, we can construct an independent approximation to the distribution by taking the product of the marginals.\n","\n","$$P_{indep}(x,y) = P(x) P(y)$$"],"metadata":{"id":"xhwQz4kzBtoY"},"id":"xhwQz4kzBtoY"},{"cell_type":"markdown","source":["Compute $P_{indep}$, defined above. Then make an image plot showing $P_{indep}$, similar to the one used to depict the original distribution."],"metadata":{"id":"ydB4Iyx-Bwfo"},"id":"ydB4Iyx-Bwfo"},{"cell_type":"code","source":["# TODO: Your code here!\n","# TODO: compute P_indep(x, y) = P(x) * P(y) using outer product\n","P_indep = ____\n","# TODO: plot P_indep as image (use same style as for Pxy)\n","____\n"],"metadata":{"id":"4TD8GtlKB0LQ"},"id":"4TD8GtlKB0LQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recall that an alternate definition of independence is that all conditional distributions are the same. Verify that the distribution $P_{indep}$ is indeed independent by computing and plotting two conditionals, $P_{indep}(x | y = 5)$ and $P_{indep}(x | y = 20)$ on the same set of axes, and showing that they agree."],"metadata":{"id":"Was5D8jDB2eY"},"id":"Was5D8jDB2eY"},{"cell_type":"code","source":["# TODO: Your code here!\n","# TODO: compute Px_given_y5_indep\n","y_index_5 = np.argmin(np.abs(y - 5))\n","Px_given_y5_indep = ____\n","\n","# TODO: compute Px_given_y20_indep\n","y_index_20 = np.argmin(np.abs(y - 20))\n","Px_given_y20_indep = ____\n","\n","# TODO: plot both conditionals on same axes\n","____"],"metadata":{"id":"ucSD2uusB5vQ"},"id":"ucSD2uusB5vQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"b61db8de","metadata":{"id":"b61db8de"},"source":["# Exercise 4 Multivariate Gaussian"]},{"cell_type":"markdown","id":"6ef44f7b","metadata":{"id":"6ef44f7b"},"source":["Generate 10000 samples from a 2D Gaussian distribution with mean `[0,0]` and covariance `[[2,0.5],[0.5,1]]`. Hint: use `np.random.multivariate_normal`"]},{"cell_type":"code","execution_count":null,"id":"6e9b2e8b","metadata":{"id":"6e9b2e8b"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","mean = ____\n","covariance = ____\n","samples = ____"]},{"cell_type":"markdown","id":"a9c42059","metadata":{"id":"a9c42059"},"source":["Check the covariance of the sample and compare it with ground true covariance"]},{"cell_type":"code","execution_count":null,"id":"f6904026","metadata":{"id":"f6904026"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","sample_cov = ____  # TODO: np.cov()\n","print(sample_cov)"]},{"cell_type":"markdown","id":"572f3b01","metadata":{"id":"572f3b01"},"source":["Estimate the probability density function by the samples. Hint: use np.histogram2d and remember the PDF should integrate to 1."]},{"cell_type":"code","execution_count":null,"id":"22debb7a","metadata":{"id":"22debb7a"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","freq, xx, yy = ____  # TODO: np.histogram2d(..., bins=100)\n","pdf = ____           # TODO: normalize so integral=1\n","plt.imshow(pdf)\n"]},{"cell_type":"markdown","id":"ed0583fc","metadata":{"id":"ed0583fc"},"source":["Sample several vectors with length 500 from a multivariate Gaussian with mean zero (zero vector of lenght $500$) and identity matrix (of size 500$\\times$500) covariance. Plot the vectors."]},{"cell_type":"code","execution_count":null,"id":"751552ef","metadata":{"id":"751552ef"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","mean = ____\n","covariance = ____\n","samples = ____\n","for i in range(len(samples)):\n","    plt.plot(samples[i])"]},{"cell_type":"markdown","id":"d9039e72","metadata":{"id":"d9039e72"},"source":["Everything is the same as the above question, but the covariance matrix is set to be $C_{ij}=i*j$. Plot the vectors"]},{"cell_type":"code","execution_count":null,"id":"3aa0948d","metadata":{"id":"3aa0948d"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","mean = ____\n","covariance = ____\n","samples = ____\n","for i in range(len(samples)):\n","    plt.plot(samples[i])"]},{"cell_type":"markdown","id":"01880e44","metadata":{"id":"01880e44"},"source":["Everything is the same as the above question, but the covariance matrix is set to be $C_{ij}=\\exp(-0.001*(i-j)^2)$. Plot the vectors"]},{"cell_type":"code","execution_count":null,"id":"6966c1e3","metadata":{"id":"6966c1e3"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","mean = ____\n","covariance = ____\n","samples = ____\n","for i in range(len(samples)):\n","    plt.plot(samples[i])"]},{"cell_type":"markdown","id":"f9f198e3","metadata":{"id":"f9f198e3"},"source":["Covariance matrix is set to be $C_{ij}=\\exp(-\\sin^2(0.1*|i-j|))$. Plot the vectors"]},{"cell_type":"code","execution_count":null,"id":"7f4b1568","metadata":{"id":"7f4b1568"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","mean = ____\n","covariance = ____\n","samples = ____\n","for i in range(len(samples)):\n","    plt.plot(samples[i])\n"]},{"cell_type":"markdown","id":"75b80444","metadata":{"id":"75b80444"},"source":["# Exercise 5 Bayes theorem"]},{"cell_type":"markdown","id":"276e7dfc","metadata":{"id":"276e7dfc"},"source":["Consider a mouse in an experimental box, with two levers to choose from, one foor food, and another for water.\n","We want to know what the mouse's preference for water is on a particular day of the week. We will use bernoulli distribution to model the mouse's preference. Let's denote $P(water)=\\theta$ and $P(food)=1-\\theta$. Our goal is to inference $\\theta$ based on the observed data. About the data, we will code choose for water as $1$ and food as $0$, so the data $d$ looks like $d=(0,1,0,1,1,1,1,\\dots)$.\n","\n","The goal is calculate\n","$$p(\\theta | d) = \\frac{P(d | \\theta) P(\\theta)}{\\int P(d | \\theta) P(\\theta) \\mathop{d \\theta}}$$\n","\n","In order to calculate the integral numerically, we are going to digitze $\\theta$ into $N$ bins with width $\\Delta_\\theta$, then\n","$$\n","P(d)= \\int P(d | \\theta) P(\\theta) \\mathop{d \\theta} \\approx \\sum_{j=1}^{N} P(d | \\Delta_\\theta*j) P(\\Delta_\\theta*j) \\Delta_{\\theta}\n","$$\n"]},{"cell_type":"markdown","id":"af0ac02f","metadata":{"id":"af0ac02f"},"source":["Let's say we've observed the mouse's choices on previous days, and have counted the number of times it choose water and the number of times it choose food. With that information we assign a prior distribution to $P(\\theta)$. Out of 86 choices it chose water 35 times, and food 51 times. We'll use this information to create a prior distribution for $\\theta$ below\n","\n"]},{"cell_type":"code","execution_count":null,"id":"81762a74","metadata":{"id":"81762a74"},"outputs":[],"source":["from scipy.stats import bernoulli,beta\n","\n","true_water_pref = 0.56 # ground truth\n","np.random.seed(45)\n","data = bernoulli(p=true_water_pref).rvs(100) # generate some data from the ground truth\n","\n","prior_water_choices = 35\n","prior_total_choices = 86\n","prior_food_choices = prior_total_choices - prior_water_choices\n","prior = lambda x: beta(prior_water_choices+1, prior_food_choices+1).pdf(x)\n","# Prior is a beta distribution\n","# Ask ChatGPT why we set our prior like this if interested\n","\n","thetas = np.linspace(0.001, 0.99, 2000) #assume a discrete grid of theta for visualization\n","all_prior=prior(thetas)\n","plt.plot(thetas,all_prior)\n","plt.xlabel(r\"$\\theta$\")\n","plt.ylabel(r\"$P(\\theta)$\")\n","plt.title(\"prior\")"]},{"cell_type":"markdown","id":"21d9da68","metadata":{"id":"21d9da68"},"source":["At what value of $\\theta$ is $P(\\theta)$ maximized? What will happen to the prior if we double `prior_water_choices` and `prior_total_choices`?"]},{"cell_type":"code","execution_count":null,"id":"6bd58dce","metadata":{"id":"6bd58dce"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","new_prior_water_choices = 35*2\n","new_prior_total_choices = 86*2\n","new_prior_food_choices = new_prior_total_choices - new_prior_water_choices\n","new_prior = beta(new_prior_water_choices+1, new_prior_food_choices+1)\n","plt.plot(thetas,new_prior.pdf(thetas))\n","plt.xlabel(r\"$\\theta$\")\n","plt.ylabel(r\"$P(\\theta)$\")\n","plt.title(\"prior\")"]},{"cell_type":"markdown","id":"68fa409d","metadata":{"id":"68fa409d"},"source":["**Likelihood**\n","\n","When you have an array of data points as we do, how do we compute $P(d | \\theta)$? The first thing to realize is that $P(d | \\theta)$ is a shortened version of:\n","\n","$$P(0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ... | \\theta)$$\n","\n","additionally we make an assumption of independence and identitically distributed. This means that we are assuming each sample uses the same distributtion for it's likelihood function, and that each sample is independent of one another (obviously this latter assumption is wrong).\n","\n","Now recall that if two events are independent then: $P(a, b) = P(a)P(b)$. Use this fact the likelihood can be factorized as\n","$$P(d | \\theta) = \\prod_{i=1}^{N} P(d_i|\\theta) = \\prod_{i=1}^{N} \\theta^{d_i}(1-\\theta)^{1-d_i}.$$\n","Write a function that takes $d$ and $\\theta$ as input and $P(d|\\theta)$ as output.Then plot $P(d|\\theta)$ against $\\theta$."]},{"cell_type":"code","execution_count":null,"id":"fcd933ae","metadata":{"id":"fcd933ae"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","def likelihood(d,theta):\n","    loglikelihood =(d*np.log(theta)+(1-d)*np.log(1.0-theta)).sum()\n","    return np.exp(loglikelihood)\n","# def likelihood(d,theta):\n","#     return bernoulli(p=theta).pmf(d).prod()\n","\n","all_likelihood = np.array([likelihood(data,theta) for theta in thetas])\n","plt.plot(thetas,all_likelihood)\n","plt.xlabel(r\"$\\theta$\")\n","plt.ylabel(r\"$P(d|\\theta)$\")\n","plt.title(\"Likelihood\")"]},{"cell_type":"markdown","id":"0053df48","metadata":{"id":"0053df48"},"source":["Does $P(d|\\theta)$ normalized with respect to $\\theta$? If not, try to normalized it. How can you interpret the meaning of normalized $P(d|\\theta)$\n","\n","Hint: Since\n","$$p(\\theta | d) = \\frac{P(d | \\theta) P(\\theta)}{\\int P(d | \\theta) P(\\theta) \\mathop{d \\theta}}$$\n","If $P(\\theta) = \\mbox{constant}$ for all $\\theta$, which is actually the uniform distribution, then  $P(\\theta)$ can be cancelled out in the denominator and numerator, we will have\n","$$p(\\theta | d) = \\frac{P(d | \\theta) }{\\int P(d | \\theta) \\mathop{d \\theta}}$$, and this is exactly the normalized likelihood. Therefore the normalized likelihood is actually the posterior with uniform prior. Alternatively, we can say that when we have uniform prior, the likelihood is actually identical to the posterior, so uniform prior is actually no prior."]},{"cell_type":"code","execution_count":null,"id":"a2c3c76c","metadata":{"id":"a2c3c76c"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","delta_theta = thetas[1]-thetas[0]\n","norm = np.sum(all_likelihood*delta_theta)\n","normed_likelihood = all_likelihood/norm\n","plt.plot(thetas,normed_likelihood)\n","plt.xlabel(r\"$\\theta$\")\n","# plt.ylabel(r\"$P(d|\\theta)$\")\n","plt.title(\"normalized likelihood\")"]},{"cell_type":"markdown","id":"f9e9796d","metadata":{"id":"f9e9796d"},"source":["**Posterior**\n","\n","Write a function that takes $d$ and $\\theta$ as input and $P(\\theta|d)$ as output.Then plot $P(\\theta|d)$ against $\\theta$ together with the normalized likelihood and prior.\n","\n","Hint1: use the approximation $P(d)= \\int P(d | \\theta) P(\\theta) \\mathop{d \\theta} \\approx \\sum_{j=1}^{N} P(d | \\Delta_\\theta*j) P(\\Delta_\\theta*j) \\Delta_{\\theta} $ to calculate the integral, where $N$ is a sufficiently large number.\n","\n","Hint2: save the result of $P(d)= \\int P(d | \\theta) P(\\theta) \\mathop{d \\theta}$, so that you don't need to re-calculate it when you need."]},{"cell_type":"code","execution_count":null,"id":"b80fb570","metadata":{"id":"b80fb570"},"outputs":[],"source":["# YOUR ANSWERS HERE\n","def evidence(d,likelihood,prior,thetas=thetas):\n","    e = np.zeros_like(thetas)\n","    delta_theta = thetas[1]-thetas[0]\n","    for i,t in enumerate(thetas):\n","        e[i] = prior(t)*likelihood(d,t)*delta_theta\n","    return np.sum(e)\n","\n","def posterior(d,theta,likelihood=likelihood,prior=prior,evidence=evidence,thetas=thetas,batch=False):\n","    if not batch:\n","        return likelihood(d,theta)*prior(theta)/evidence(d,likelihood,prior,thetas=thetas)\n","    else:\n","        ev = evidence(d,likelihood,prior,thetas=thetas)\n","        return np.array([likelihood(d,theta)*prior(theta)/ev for theta in thetas])\n","\n","\n","all_posterior = posterior(data,thetas,batch=True)\n","plt.plot(thetas,all_posterior,label=\"posterior\")\n","plt.plot(thetas,normed_likelihood,label=\"normalized likelihood\")\n","plt.plot(thetas,all_prior,label=\"prior\")\n","plt.axvline(true_water_pref, linestyle=\"--\", color=\"black\", label=\"true water pref\")\n","plt.legend()\n","plt.xlabel(r\"$\\theta$\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[{"file_id":"1Mye_IwulPtcNX3u9tAbhB_YD5_jrK8MK","timestamp":1754969416784},{"file_id":"1DzHawnP8hT9BZ8BysJrDSNQ5UsrpLE7u","timestamp":1754961295208},{"file_id":"1zXBsxef18v5xJUQCNKYBsqohdmMgoK2W","timestamp":1753202121850}]}},"nbformat":4,"nbformat_minor":5}